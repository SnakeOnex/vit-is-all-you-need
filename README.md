# ViT (Vision Transformer) is all you need

The goal is to do a minimal ViT implementation and then be able to reproduce many of the ViT based papers with just a few lines of code.

## Papers
- [ ] [ViT - An Image is Worth 16x16 words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [ ] [TiTok - An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)
- [ ] [MAE - Masked Autoencoders are scalable Vision Transformers](https://arxiv.org/abs/2012.12877)
- [ ] [Puzzle - Position Prediction as an Effective Pretraining Task for Object Detection](https://arxiv.org/abs/2207.07611)


